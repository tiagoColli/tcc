<h2>Submodules</h2>
<h2>Module contents</h2>
<dl>
<dt><ac:structured-macro ac:name="anchor">
<ac:parameter ac:name="">willfsemr.Writer</ac:parameter>
</ac:structured-macro>
<em>class </em><code>willfsemr.</code><strong><code>Writer</code></strong>(<em>environment: str = &apos;pag&apos;</em>, <em>local=False</em>)</dt>
<dd><p>Bases: <code>object</code></p>
<p>The Writer class is responsible to interface the python3         and the feature store.</p>
<p>Construction of the Writer class.</p>
<dl>
<dt>Args:</dt>
<dd>
<p><strong>environment</strong> (bool, optional): Define te environment that will  be used by feature store, defaults to pag. Possible values are: [‘pag’, ‘will-prod’, ‘will-dev’, ‘local’]</p>
<p><strong>local</strong> (bool, optional): changes the spark config - set to false when running the Reader inside the EMR</p>
</dd>
<dt>Return:</dt>
<dd>
<p><strong>Writer</strong> object</p>
</dd>
</dl>
<dl>
<dt><ac:structured-macro ac:name="anchor">
<ac:parameter ac:name="">willfsemr.Writer.save_to_feature_store</ac:parameter>
</ac:structured-macro>
<strong><code>save_to_feature_store</code></strong>(<em>dataset</em>, <em>grouping_key</em>, <em>features_to_save</em>, <em>date_column=&apos;snapshotdate&apos;</em>, <em>partition=False</em>, <em>cast_date=False</em>, <em>feature_group=&apos;generic&apos;</em>, <em>mode=&apos;overwrite&apos;</em>, <em>remove_existing=False</em>)</dt>
<dd><dl>
<dt>Receives the dataframe or dataset, formats and uploads the data to </dt>
<dd>
<p>the Feature Store.</p>
</dd>
<dt>Args:</dt>
<dd>
<p><strong>dataset</strong> ([Pandas or Spark dataset]): Dataset containing the  data to be saved.</p>
<p><strong>grouping_key</strong> (str): A string containing the  grouping key of the dataset.</p>
<p><strong>features_to_save</strong> (string list): A list containing the  dataset’s features to be saved. Grouping key and date column  cant be here</p>
<p><strong>date_column</strong> (str, optional): Name of the snapshot date column. If the partition flag is set, it will occur based on  the name of this column. Defaults to ‘snapshotdate’.</p>
<p><strong>partition</strong> (bool, optional): Flag to turn on the partition  of the parquet file saved to feature store. Defaults to False.</p>
<p><strong>cast_date</strong> (bool, optional): If set, the Writer will force  the format YYYY-MM-DD to the date column. Defaults to False.</p>
<p><strong>feature_group</strong> (str, optional): Group in which the features is  part of. Possible values are [behavior, collection,  application, generic, test]. Defaults to ‘generic’.</p>
<p><strong>mode</strong> (string, optional): Writing mode to perform the insertion to S3 operation: <em>overwrite</em> or <em>append</em>. Defaults ‘overwrite’</p>
<p><strong>remove_existing</strong> (bool, optional): If set, the Writer will load  the existing feature, compare the snapshots dates and performs  an anti-left join. The result will be written to the database. Defaults to False.</p>
</dd>
<dt>Return:</dt>
<dd>
<p><strong>None</strong></p>
</dd>
</dl>
</dd>
</dl>
<dl>
<dt><ac:structured-macro ac:name="anchor">
<ac:parameter ac:name="">willfsemr.Writer.unload_from_athena</ac:parameter>
</ac:structured-macro>
<strong><code>unload_from_athena</code></strong>(<em>sql_query</em>, <em>database</em>)</dt>
<dd><p>A function to unload data from Amazon Athena Databases using the AWS account Glue catalog.</p>
<p>Currently, this function is strictly to spark usage on EMR, and, therefore, the output is an Spark Dataframe. The query should not contain “;”. Use one query at a time.</p>
<dl>
<dt>Args:</dt>
<dd>
<p><strong>sql_query</strong> (str): Query to be executed in the Athena databases.</p>
<p><strong>database</strong> (str): Database name to the Athena connection.</p>
</dd>
<dt>Returns:</dt>
<dd>
<p><strong>dataframe</strong>: Returns an Spark dataframe.</p>
</dd>
</dl>
</dd>
</dl>
<dl>
<dt><ac:structured-macro ac:name="anchor">
<ac:parameter ac:name="">willfsemr.Writer.unload_from_redshift</ac:parameter>
</ac:structured-macro>
<strong><code>unload_from_redshift</code></strong>(<em>sql_query</em>, <em>**connectionArgs</em>)</dt>
<dd><p>A function to unload data from Amazon Redshift Database.</p>
<p>Currently, this function is strictly to pandas usage, and, therefore, the output is a pandas dataframe. If needed, initialize a Writer with spark parameter set to false and perform the convertion manually.</p>
<dl>
<dt>Args:</dt>
<dd>
<p><strong>sql_query</strong> (str): Query to be executed in the Redshift database.</p>
<p><strong>username</strong> (str): Username to the Redshift database connection.</p>
<p><strong>password</strong> (str): Password to the Redshift database connection.</p>
<p><strong>host</strong> (str): Host addres of the Redshift database.</p>
<p><strong>database</strong> (str): Database name to the Redshift connection.</p>
</dd>
<dt>Returns:</dt>
<dd>
<p><strong>dataframe</strong>: Returns a pandas dataframe.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
